#!/usr/bin/env python3
"""
è¶…å¿«é€Ÿæ¨¡å‹è¨“ç·´ - åƒ…ä½¿ç”¨æœ€å„ªæ¨¡å‹é…ç½®

å°ˆæ³¨æ–¼æœ€æœ‰æ•ˆçš„æ¨¡å‹ï¼Œè·³éå¯¦é©—æ€§é…ç½®ï¼Œå¿«é€Ÿç”¢ç”Ÿç”Ÿç”¢æ¨¡å‹ã€‚
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path

# Add project root to path
sys.path.insert(
    0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../../src"))
)

import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor

from alphapulse.ml.training.iterative_trainer import IterativeTrainer, TrainingConfig


def ultra_fast_training():
    """è¶…å¿«é€Ÿè¨“ç·´ - åƒ…æœ€å„ªé…ç½®"""

    print("=" * 80)
    print("âš¡ è¶…å¿«é€Ÿç”Ÿç”¢æ¨¡å‹è¨“ç·´")
    print("=" * 80)
    print(f"â° é–‹å§‹: {datetime.now().strftime('%H:%M:%S')}\n")

    # æ¥µè‡´å„ªåŒ–é…ç½®
    config = TrainingConfig(
        data_source="model_features",
        target_column="price_change_1d",
        min_samples_required=300,
        n_iterations=3,  # åªè¨“ç·´ 3 å€‹æœ€å„ªæ¨¡å‹
        cv_splits=3,
        early_stopping_rounds=20,
        early_stopping_patience=1,
        max_train_val_gap=0.20,  # ç¨å¾®æ”¾å¯¬ä»¥åŠ å¿«é€Ÿåº¦
        max_val_test_gap=0.15,
        min_val_r2=0.01,
        mlflow_uri=os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5000"),
        experiment_name="ultra_fast_production",
        output_dir="storage/models/saved",
        save_all_iterations=False,
    )

    print("âš™ï¸  é…ç½®: 3 è¿­ä»£ | 3 æŠ˜ CV | å¿«é€Ÿæ—©åœ")
    print("ğŸ¯ ç›®æ¨™: å¿«é€Ÿç”¢ç”Ÿå¯ç”¨æ¨¡å‹\n")

    # å‰µå»ºè¨“ç·´å™¨ä¸¦è¦†å¯«æ¨¡å‹é…ç½®ç‚ºåƒ…æœ€å„ªæ¨¡å‹
    trainer = IterativeTrainer(config)

    # æ‰‹å‹•è¨­ç½®æœ€å„ªæ¨¡å‹é…ç½®
    best_configs = [
        {
            "name": "xgboost_balanced",
            "class": xgb.XGBRegressor,
            "params": {
                "n_estimators": 100,
                "max_depth": 3,
                "learning_rate": 0.05,
                "reg_alpha": 0.1,
                "reg_lambda": 1.0,
                "subsample": 0.8,
                "colsample_bytree": 0.8,
                "random_state": 42,
            },
        },
        {
            "name": "xgboost_conservative",
            "class": xgb.XGBRegressor,
            "params": {
                "n_estimators": 80,
                "max_depth": 2,
                "learning_rate": 0.03,
                "reg_alpha": 0.2,
                "reg_lambda": 2.0,
                "subsample": 0.8,
                "colsample_bytree": 0.8,
                "random_state": 42,
            },
        },
        {
            "name": "random_forest_balanced",
            "class": RandomForestRegressor,
            "params": {
                "n_estimators": 100,
                "max_depth": 5,
                "min_samples_leaf": 10,
                "min_samples_split": 20,
                "max_features": "sqrt",
                "n_jobs": -1,
                "random_state": 42,
            },
        },
    ]

    # è¦†å¯«ç”Ÿæˆæ¨¡å‹é…ç½®çš„æ–¹æ³•
    trainer.generate_model_configs = lambda: best_configs

    print("ğŸš€ é–‹å§‹è¨“ç·´...\n")

    try:
        summary = trainer.run_iterative_training()

        print("\n" + "=" * 80)
        print("âœ… å®Œæˆï¼")
        print("=" * 80)

        if summary.get("best_model"):
            best = summary["best_model"]
            print(f"\nğŸ† æœ€ä½³: {best['name']}")
            print(f"   Val MAE: {best['val_mae']:.6f}")
            print(f"   Test MAE: {best['test_mae']:.6f}")
            print(f"   Test RÂ²: {best['test_r2']:.4f}")

        print(f"\nâ° å®Œæˆ: {datetime.now().strftime('%H:%M:%S')}")
        print(f"ğŸ’¾ æ¨¡å‹: {config.output_dir}/best_model.pkl")

        return summary

    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {str(e)}")
        import traceback

        traceback.print_exc()
        return None


if __name__ == "__main__":
    summary = ultra_fast_training()
    sys.exit(0 if summary else 1)
